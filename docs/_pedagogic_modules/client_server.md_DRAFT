

The goal of this module is to introduce you to the considerations for a client/server setup.


#### Learning Objectives:

  - Understand client/server functionality

  - Understand how to optimize a client/server setup

----

## Basics

In a client/server model there is a client which can house data and receive inputs, while one or many servers are
networked with the client in order to process requests or tasks. This setup allows both client and server hardware some level of
specialization and autonomy. Many applications and websites are clients, where they receive information from the end
user and forward their request to a server for actual processing. This can keep the application lightweight and widely
accessible, while still allowing for resource-intensive activities to be done on the server at client request. There are
 drawbacks that need to be taken into account for this architecture. If a substantial amount of data needs to be
 transferred over the network from the client to server or vice versa, this will be constrained by network speeds. As
 servers can accommodate multiple clients in many cases, there may also be issues with load on the server from trying to
  handle multiple requests at once. Managing these and other restraints is necessary when running a client/server architecture.


### An Example: Photo Processing

You have an application that takes photos from your local client and sends them to a server for image processing. The
images are sent in batches of 100 that take up 100 MB of space. Currently the client is on a machine with an HDD capable
of R/W at 50 MBps, and it has the option of using two different servers. Server 1 has a link to the client that runs at only
10 MBps, but it can compute 100 GFlop/s. Server 2 has a 100 MBps link but it is able to compute at just 10 GFlop/s.
Latency in this situation is negligible and can be disregarded, and the server loads the images directly into its RAM,
they are not written to disk on the server.

In this situation, you can see that the disk is slower than the available network speed. As previously observed in the
IO module of Single-Core Computing, a slow disk is often the bottleneck in computing. In this situation, we could
accelerate the process by updating the disk of our client machine. An upgrade to link speed would not help as it can
already transfer data twice as fast as the disk can read.


<object class="figure" type="image/svg+xml" data="{{ site.baseurl }}/public/img/client_server/client_server.svg">Client / Server Topology</object>


### Simulating a Client and Server

So that you can gain hands-on experience, use
the simulation Web application
(see <a href="{{site.baseurl}}/pedagogic_modules/simulation_instructions/index/" target="_blank">instructions</a>),
selecting `Client/Server` from its menu.

This simulation app allows you to see the differences in execution time when you trade off between slow and fast networks
and disks. First, you can try running the simulation using Server 1 or Server 2 (check the checkbox for Server 1, leave
it unchecked for Server 2). Run with the default values and you will notice a large difference in execution time. Even
though Server 1 has a better CPU, it is bottlenecked by the speed at which it can receive data over the link. Server 2
is thus able to finish execution much more quickly than Server 1.


### Buffering

Buffering is a way to improve performance for large data transfers. Without a buffer you can imagine a constant data
stream over the network, 1 bit at a time. If we implement a buffer of 4 kilobytes instead, the disk would instead wait
to read 4K before sending it over the network. These 4K chunks would be sent until the transfer was finished. This
provides several advantages for efficiency. A larger buffer means that the network link can maintain a consistent transfer
 for the duration of that chunk. If a tiny stream of data is sent as it is read from disk, there can be gaps where the
 network is waiting. If, as in most cases, the link is not dedicated to only this traffic, there will also need to be
 other traffic taking up link capacity at times. Sending data in tiny chunks will also increase overhead at all stages
 of the process. There is a limit on how large buffer should be, as whether it is buffering in RAM or dedicated memory
 there will be an upper limit on what it can store at any given time.


#### Practice Questions


**[X.p1.1]** You have a task that needs to execute on a server. This task requires 400 MB of input to run, and it must be
transferred from the client's disk to the server's RAM. The client disk has a R/W speed of 200 MBps and there is a 1 GBps
network link between the client an server. Latency is negligible and can be disregarded. The task is 1 TFlop and the server's
CPU is capable of 200 GFlop/second. The task can only begin when all input data is available in RAM. For this question,
assume there is no buffering, as soon as data is read from disk it can be sent on the network link utilizing the full
bandwidth. How long is the execution time from start to finish?

<div class="ui accordion fluid">
  <div class="title">
    <i class="dropdown icon"></i>
    (click to see answer)
  </div>
  <div markdown="1" class="ui segment content">
    The 400MB will take 2 seconds to be read from disk. The network link is faster than the disk, so the only additional
     transfer time will be latency which we have been told is negligible. Once the data is on the server, it can complete
      the task in 5 seconds. Total execution time will be 2+5 = 7 seconds.

  </div>
</div>

<p></p>

**[X.p1.2]** Consider the previous question's situation, but now the server has moved and the network link has changed
to 10 GBps capacity. Due to the longer distance, there is now 100 μs latency. Does this change the execution time?

<div class="ui accordion fluid">
  <div class="title">
    <i class="dropdown icon"></i> (click to see answer)
  </div> <div markdown="1" class="ui segment content">
   Compared to the previous answer, upgrading the bandwidth of the network link does nothing as it was never fully
   utilized to begin with. Since we are going from negligible latency to 100 μs latency, our answer would be increased
   by that amount. Transfer time will extend past the time it takes to read from disk by the amount of latency.

  </div>
</div>

<p></p>



#### Questions